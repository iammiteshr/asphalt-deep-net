{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6215409d",
   "metadata": {},
   "source": [
    "# Pothole Detection â€” YOLOv8 Quickstart\n",
    "This notebook trains a **pothole detector** with **YOLOv8**.\n",
    "It expects data in YOLO format with a single class `pothole`. See README for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564e50c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install deps (uncomment if running locally)\n",
    "# !pip install -r ../requirements.txt\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import os, json, glob, shutil, random\n",
    "from pathlib import Path\n",
    "print(\"Ultralytics version:\", YOLO.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07545b65",
   "metadata": {},
   "source": [
    "## 1) (Optional) Convert Pascal VOC XML to YOLO\n",
    "If your dataset uses VOC XML, run the converter. Adjust paths as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb96ee47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example: convert VOC to YOLO (edit the paths before running)\n",
    "# %run ../src/voc_to_yolo.py --images /path/to/images --ann /path/to/xmls --out ../data --split 0.85 --only pothole --copy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4ff5b4",
   "metadata": {},
   "source": [
    "## 2) Verify dataset layout\n",
    "Expected: `../data/images/{train,val}` and `../data/labels/{train,val}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ff4e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "root = Path(\"..\")\n",
    "for p in [\"data/images/train\",\"data/images/val\",\"data/labels/train\",\"data/labels/val\"]:\n",
    "    pth = root/p\n",
    "    print(pth, \"exists?\" , pth.exists())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93229e2f",
   "metadata": {},
   "source": [
    "## 3) Train YOLOv8 (detection)\n",
    "Start with a small model (`yolov8n.pt`) and iterate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0e9f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "data_cfg = str((Path(\"..\")/\"configs\"/\"data.yaml\").resolve())\n",
    "model = YOLO(\"yolov8n.pt\")  # nano model to start; try yolov8s.pt later\n",
    "results = model.train(data=data_cfg, epochs=50, imgsz=1024, batch=8, device=0)\n",
    "model.val()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d18a27c",
   "metadata": {},
   "source": [
    "## 4) Inference demo on a sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb4b148",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2, matplotlib.pyplot as plt\n",
    "\n",
    "# Put a test image under ../data/images/val/ to visualize\n",
    "val_imgs = list((Path(\"..\")/\"data\"/\"images\"/\"val\").glob(\"*.*\"))\n",
    "if val_imgs:\n",
    "    test_im = str(val_imgs[0])\n",
    "    res = model.predict(source=test_im, conf=0.35, imgsz=1024)[0]\n",
    "    im_bgr = res.plot()  # annotated\n",
    "    im_rgb = cv2.cvtColor(im_bgr, cv2.COLOR_BGR2RGB)\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.imshow(im_rgb); plt.axis('off')\n",
    "else:\n",
    "    print(\"No images found under data/images/val\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971fa40d",
   "metadata": {},
   "source": [
    "## 5) Export for deployment (pick one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5c02d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ONNX\n",
    "model.export(format=\"onnx\")\n",
    "\n",
    "# TensorRT (requires environment support)\n",
    "# model.export(format=\"engine\")\n",
    "\n",
    "# CoreML (macOS / iOS dev)\n",
    "# model.export(format=\"coreml\")\n",
    "\n",
    "# TFLite\n",
    "# model.export(format=\"tflite\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e8b752",
   "metadata": {},
   "source": [
    "## (Optional) Severity scoring with monocular depth (MiDaS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21b7bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pip install timm\n",
    "\n",
    "import torch, numpy as np, cv2, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Load MiDaS\n",
    "midas = torch.hub.load(\"intel-isl/MiDaS\", \"DPT_Hybrid\")\n",
    "transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
    "transform = transforms.dpt_transform\n",
    "midas.eval()\n",
    "\n",
    "def mean_depth_inside_mask(depth_map, mask_xy):\n",
    "    m = np.zeros(depth_map.shape, dtype=np.uint8)\n",
    "    cv2.fillPoly(m, [mask_xy.astype(np.int32)], 1)\n",
    "    vals = depth_map[m==1]\n",
    "    return float(vals.mean()) if vals.size else 0.0\n",
    "\n",
    "# Example: run on one prediction and compute mean depth within each box area (approximate using boxes)\n",
    "val_imgs = list((Path(\"..\")/\"data\"/\"images\"/\"val\").glob(\"*.*\"))\n",
    "if val_imgs:\n",
    "    test_im = cv2.imread(str(val_imgs[0]))\n",
    "    inp = transform(cv2.cvtColor(test_im, cv2.COLOR_BGR2RGB)).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        depth = midas(inp).squeeze().cpu().numpy()\n",
    "    depth = (depth - depth.min()) / (depth.max() - depth.min() + 1e-6)\n",
    "\n",
    "    res = model.predict(source=str(val_imgs[0]), conf=0.35, imgsz=1024)[0]\n",
    "    scores = []\n",
    "    for b in res.boxes.xyxy.cpu().numpy():\n",
    "        x1,y1,x2,y2 = b.astype(int)\n",
    "        mask_xy = np.array([[x1,y1],[x2,y1],[x2,y2],[x1,y2]])\n",
    "        area = (x2-x1)*(y2-y1)\n",
    "        md = mean_depth_inside_mask(depth, mask_xy)\n",
    "        scores.append({\"bbox\": [int(x) for x in [x1,y1,x2,y2]], \"area_px\": int(area), \"mean_depth\": md})\n",
    "    print(json.dumps(scores, indent=2))\n",
    "else:\n",
    "    print(\"Add a sample to data/images/val first.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
